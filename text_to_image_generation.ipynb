{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYWxkcRVyKNvHQ30DQrM+V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrew2003-error/tic-tac-toe-python/blob/main/text_to_image_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RlqNIqCnh_gU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "outputId": "f2b0ffb7-763b-492d-de5d-4b6f5d37405b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7050bfb58b3b1aa856.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7050bfb58b3b1aa856.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Example 2.2\\n# Image Caption Generation\\n#Install the necessary libraries\\n\\n!pip install transformers\\n!pip install gradio\\nforce_download=True\\n# Build the Image Captioning Pipeline\\n\\nfrom transformers import pipeline\\nimage_captioner =pipeline(\"image-to-text\",model=\"Salesforce/blip-image-captioning-large\")\\n# Set up Prerequisites for Image Captioning App User Interface\\n\\nimport os\\nimport io\\nimport IPython.display\\nfrom PIL import Image\\nimport base64\\n\\nimport gradio as gr\\n\\ndef image_to_base64_str(pil_image):\\n    byte_arr = io.BytesIO()\\n    pil_image.save(byte_arr, format=\\'PNG\\')\\n    byte_arr = byte_arr.getvalue()\\n    return str(base64.b64encode(byte_arr).decode(\\'utf-8\\'))\\ndef captioner(image):\\n    base64_image = image_to_base64_str(image)\\n    result = image_captioner(base64_image)\\n    return result[0][\\'generated_text\\']\\n\\ngr.close_all()\\n# Build the Image Captioning App and Launch\\n\\nImageCaptionApp = gr.Interface(fn=captioner,\\n                    inputs=[gr.Image(label=\"Upload image\", type=\"pil\")],\\n                    outputs=[gr.Textbox(label=\"Caption\")],\\n                    title=\"Image Captioning with BLIP\",\\n                    description=\"Caption any image using the BLIP model\",\\n                    allow_flagging=\"never\")\\n\\nImageCaptionApp.launch()\\n\\n# Example 2.3\\n# Face Mask Detection\\n!pip install gradio transformers Pillow\\n\\n# Use a pipeline as a high-level helper\\nfrom transformers import pipeline\\nfrom PIL import Image\\npipe = pipeline(\"image-classification\", model=\"AkshatSurolia/ConvNeXt-FaceMask-Finetuned\")\\n\\n# Load model directly\\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\\n\\nprocessor = AutoImageProcessor.from_pretrained(\"AkshatSurolia/ConvNeXt-FaceMask-Finetuned\")\\nmodel = AutoModelForImageClassification.from_pretrained(\"AkshatSurolia/ConvNeXt-FaceMask-Finetuned\")\\n\\n# Define a function to process and classify the uploaded image\\ndef classify_image(image):\\n    # Convert the image to a format that the model can handle\\n    img = Image.fromarray(image)\\n\\n    # Use the pipeline to predict\\n    predictions = pipe(img)\\n\\n    # Return the top prediction\\n    return {pred[\\'label\\']: pred[\\'score\\'] for pred in predictions}\\n\\nimport gradio as gr\\n# Create a Gradio interface\\niface = gr.Interface(\\n    fn=classify_image,  # The function for prediction\\n    inputs=gr.Image(type=\"numpy\"),  # Input as an image, Use gr.Image directly\\n    outputs=\"label\",  # Output the label and score\\n    title=\"Face Mask Classification\",\\n    description=\"Upload an image to classify whether a face is wearing a mask or not using ConvNeXt.\"\\n)\\n\\n# Launch the interface\\niface.launch()\\n\\n# Example 2.4\\n# Object Detection in an Image\\n!pip install gradio transformers torch matplotlib\\nimport gradio as gr\\nfrom transformers import DetrFeatureExtractor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.patches import Rectangle\\nimport numpy as np\\n\\n# Load the model and feature extractor\\nmodel_name = \"facebook/detr-resnet-50\"\\nfeature_extractor = DetrFeatureExtractor.from_pretrained(model_name)\\nmodel = DetrForObjectDetection.from_pretrained(model_name)\\n\\n# Function to perform object detection and return an image with boxes\\ndef detect_objects(image):\\n    # Prepare the image for the model\\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\\n\\n    # Perform inference\\n    outputs = model(**inputs)\\n\\n    # Get the detected boxes and labels\\n    target_sizes = torch.tensor([image.size[::-1]])  # Model expects (height, width)\\n    results = feature_extractor.post_process(outputs, target_sizes=target_sizes)[0]\\n\\n    # Visualize the results\\n    plt.imshow(image)\\n    ax = plt.gca()\\n\\n    for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\\n        if score > 0.9:  # Filter by confidence threshold\\n            box = box.detach().numpy()\\n            x, y, w, h = box\\n            rect = Rectangle((x, y), w - x, h - y, linewidth=2, edgecolor=\\'red\\', facecolor=\\'none\\')\\n            ax.add_patch(rect)\\n            label_text = f\"{model.config.id2label[label.item()]}: {round(score.item(), 3)}\"\\n            plt.text(x, y, label_text, color=\\'white\\', fontsize=12, bbox=dict(facecolor=\\'red\\', alpha=0.5))\\n\\n    plt.axis(\\'off\\')\\n\\n    # Save the image with bounding boxes\\n    plt.savefig(\"detected_image.png\", bbox_inches=\\'tight\\')\\n    plt.close()\\n\\n    return Image.open(\"detected_image.png\")\\n\\n# Gradio interface\\ninterface = gr.Interface(\\n    fn=detect_objects,\\n    inputs=gr.Image(type=\"pil\"),\\n    outputs=gr.Image(type=\"pil\"),\\n    title=\"Object Detection\",\\n    description=\"Upload an image and the model will detect objects with bounding boxes.\",\\n)\\n\\n# Launch the app\\ninterface.launch()\\n\\n#Example 2.5\\n# Generating the image of a Tourist Spot\\n# Step 0\\n! pip install gradio\\n!pip install pydub\\n\\n# Step 1\\n# Imports\\nimport gradio as gr\\nfrom io import BytesIO\\nfrom PIL import Image\\nimport base64\\nfrom pydub import AudioSegment\\nfrom pydub.playback import play\\nimport IPython.display as ipd\\nfrom openai import OpenAI\\nfrom google.colab import userdata\\nopenai_api_key = userdata.get(\\'cm_muthu\\')\\nopenai = OpenAI(api_key = openai_api_key)\\n\\n# Step 2: Define a Function to generate an image of a Tourist Spot using DALL-E-3 Model\\ndef artist(city):\\n   image_response = openai.images.generate(model = \"dall-e-3\",prompt = f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant style\",size = \"1024x1024\",n = 1,response_format = \"b64_json\", )\\n   image_base64 = image_response.data[0].b64_json\\n   image_data = base64.b64decode(image_base64)\\n   return Image.open(BytesIO(image_data))\\n\\n# Step 3 : Generate an image of a Tourist Spot using DALL-E-3 Model\\nimage = artist(\"london\")\\ndisplay(image)\\n\\n\\n#Example 2.6\\n# Multimodal AI Assistant(Text, Image, and Audio are generated)\\n# Lessons 62\\n# Step 0\\n! pip install gradio\\n!pip install pydub\\n\\n# Step 1\\n# Imports\\nimport gradio as gr\\nfrom io import BytesIO\\nfrom PIL import Image\\nimport base64\\nfrom pydub import AudioSegment\\nfrom pydub.playback import play\\nimport IPython.display as ipd\\nfrom openai import OpenAI\\nfrom google.colab import userdata\\nopenai_api_key = userdata.get(\\'cm_muthu\\')\\nopenai = OpenAI(api_key = openai_api_key)\\n\\n# Step 1A: Define a function to get ticket price for destination city\\nticket_prices = {\"london\" : \"$799\", \"paris\" : \"$899\", \"tokyo\" : \"$1400\", \"berlin\" : \"$499\"}\\ndef get_ticket_price(destination_city):\\n  # print(f\"Tool get_ticket_price called for {destination_city}\")\\n  city = destination_city.lower()\\n  return ticket_prices.get(city, \"Unknown\")\\n\\n# print(get_ticket_price(\"london\"))\\n\\n# Step 2: Define a function to generate an image\\ndef artist(city):\\n   image_response = openai.images.generate(model = \"dall-e-3\",prompt = f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant style\",size = \"1024x1024\",n = 1,response_format = \"b64_json\", )\\n   image_base64 = image_response.data[0].b64_json\\n   image_data = base64.b64decode(image_base64)\\n   return Image.open(BytesIO(image_data))\\n# image = artist(\"london\")\\n# display(image)\\n\\n# Step 3: Define a function to generate audio\\ndef talker (city , price):\\n  message = f\"Ticket price for {city} is {price}\"\\n  response = openai.audio.speech.create(model = \"tts-1\", voice = \"onyx\", input = message)\\n  audio_stream = BytesIO(response.content)\\n  audio = AudioSegment.from_file(audio_stream, format = \"mp3\")\\n  audio.export(\"output.mp3\", format = \"mp3\")\\n  play(audio)\\n# talker(\"london\" , \"$799\")\\n\\n# Step 4: Define generate_output() function\\ndef generate_output(city_name):\\n   ticket_price = get_ticket_price(city_name)\\n   message = f\"Ticket price for {city_name} is {ticket_price}\"\\n   image = artist(city_name)\\n   talker(city_name , ticket_price)\\n   return message , image\\n# message, image = generate_output(\"london\")\\n# display(image)\\n\\n# Step 5: Create Gradio Interface\\nwith gr.Blocks() as interface:\\n  destination_city = gr.Textbox(label = \"Type Destination City name\", placeholder =\"e.g.London\")\\n  ticket_price = gr.Textbox(label = \"Ticket Price\")\\n  destination_city_image = gr.Image(label = \"Destination City Image\")\\n  destination_city.submit(fn = generate_output, inputs = destination_city, outputs = [ticket_price, destination_city_image])\\n\\ninterface.launch()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Example 2.1 : Text-to-Image Generation\n",
        "# Install the Monster API Python Package\n",
        "\n",
        "!pip install gradio monsterapi -q\n",
        "import gradio as gr\n",
        "from monsterapi import client\n",
        "# Initialize the Monster API client with your API key\n",
        "\n",
        "api_key = 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6Ijg1OGI0MTIwYzQ0YzZkODI5MjgzZDM0NjFlMWY0YWRhIiwiY3JlYXRlZF9hdCI6IjIwMjQtMTAtMDlUMTQ6MzU6MTcuNTYxMTkwIn0.koOHRCm9xJHZIExKjgSdhIpFL12uOhcApcJyS2DfBKU'  # Replace with your actual Monster API key\n",
        "monster_client = client(api_key)\n",
        "\n",
        "# Define function to generate image\n",
        "\n",
        "def generate_image(prompt, style):\n",
        "    model = 'txt2img'  # Replace with the desired model name\n",
        "    input_data = {\n",
        "        'prompt': f'{prompt}, {style}',  # Combine prompt and style\n",
        "        'negprompt': 'deformed, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, disgusting, poorly drawn hands, missing limb, floating limbs, disconnected limbs, malformed hands, blurry, mutated hands, fingers',\n",
        "        'samples': 3,\n",
        "        'enhance': False,\n",
        "        'optimize': False,\n",
        "        'safe_filter': False,\n",
        "        'steps': 50,\n",
        "        'aspect_ratio': 'square',\n",
        "        'guidance_scale': 5.5,\n",
        "    }\n",
        "\n",
        "    # Call Monster API to generate image\n",
        "    result = monster_client.generate(model, input_data)\n",
        "\n",
        "    # Return the generated image URL\n",
        "    return result['output'][0]\n",
        "\n",
        "# Create Gradio interface\n",
        "\n",
        "with gr.Blocks() as ImageGenerator:\n",
        "    gr.Markdown(\"## Text-to-Image Generator with Monster API\")\n",
        "\n",
        "    prompt_input = gr.Textbox(label=\"Enter your prompt\", placeholder=\"e.g. a girl in red dress\")\n",
        "    style_input = gr.Dropdown(\n",
        "        choices=[\"watercolor\", \"photorealistic\", \"no style\", \"enhance\", \"anime\",\n",
        "                 \"photographic\", \"digital-art\", \"comic-book\", \"fantasy-art\",\n",
        "                 \"analog-film\", \"neonpunk\", \"isometric\", \"lowpoly\", \"origami\",\n",
        "                 \"line-art\", \"craft-clay\", \"cinematic\", \"3d-model\", \"pixel-art\",\n",
        "                 \"texture\", \"futuristic\", \"realism\"],\n",
        "        label=\"Choose a style\"\n",
        "    )\n",
        "    output_image = gr.Image(label=\"Generated Image\")\n",
        "\n",
        "    generate_btn = gr.Button(\"Generate Image\")\n",
        "\n",
        "    # Set the function to be called on button click\n",
        "    generate_btn.click(fn=generate_image, inputs=[prompt_input, style_input], outputs=output_image)\n",
        "\n",
        "# Launch the Gradio interface\n",
        "ImageGenerator.launch()\n",
        "\n",
        "'''# Example 2.2\n",
        "# Image Caption Generation\n",
        "#Install the necessary libraries\n",
        "\n",
        "!pip install transformers\n",
        "!pip install gradio\n",
        "force_download=True\n",
        "# Build the Image Captioning Pipeline\n",
        "\n",
        "from transformers import pipeline\n",
        "image_captioner =pipeline(\"image-to-text\",model=\"Salesforce/blip-image-captioning-large\")\n",
        "# Set up Prerequisites for Image Captioning App User Interface\n",
        "\n",
        "import os\n",
        "import io\n",
        "import IPython.display\n",
        "from PIL import Image\n",
        "import base64\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def image_to_base64_str(pil_image):\n",
        "    byte_arr = io.BytesIO()\n",
        "    pil_image.save(byte_arr, format='PNG')\n",
        "    byte_arr = byte_arr.getvalue()\n",
        "    return str(base64.b64encode(byte_arr).decode('utf-8'))\n",
        "def captioner(image):\n",
        "    base64_image = image_to_base64_str(image)\n",
        "    result = image_captioner(base64_image)\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "gr.close_all()\n",
        "# Build the Image Captioning App and Launch\n",
        "\n",
        "ImageCaptionApp = gr.Interface(fn=captioner,\n",
        "                    inputs=[gr.Image(label=\"Upload image\", type=\"pil\")],\n",
        "                    outputs=[gr.Textbox(label=\"Caption\")],\n",
        "                    title=\"Image Captioning with BLIP\",\n",
        "                    description=\"Caption any image using the BLIP model\",\n",
        "                    allow_flagging=\"never\")\n",
        "\n",
        "ImageCaptionApp.launch()\n",
        "\n",
        "# Example 2.3\n",
        "# Face Mask Detection\n",
        "!pip install gradio transformers Pillow\n",
        "\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "pipe = pipeline(\"image-classification\", model=\"AkshatSurolia/ConvNeXt-FaceMask-Finetuned\")\n",
        "\n",
        "# Load model directly\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"AkshatSurolia/ConvNeXt-FaceMask-Finetuned\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"AkshatSurolia/ConvNeXt-FaceMask-Finetuned\")\n",
        "\n",
        "# Define a function to process and classify the uploaded image\n",
        "def classify_image(image):\n",
        "    # Convert the image to a format that the model can handle\n",
        "    img = Image.fromarray(image)\n",
        "\n",
        "    # Use the pipeline to predict\n",
        "    predictions = pipe(img)\n",
        "\n",
        "    # Return the top prediction\n",
        "    return {pred['label']: pred['score'] for pred in predictions}\n",
        "\n",
        "import gradio as gr\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=classify_image,  # The function for prediction\n",
        "    inputs=gr.Image(type=\"numpy\"),  # Input as an image, Use gr.Image directly\n",
        "    outputs=\"label\",  # Output the label and score\n",
        "    title=\"Face Mask Classification\",\n",
        "    description=\"Upload an image to classify whether a face is wearing a mask or not using ConvNeXt.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()\n",
        "\n",
        "# Example 2.4\n",
        "# Object Detection in an Image\n",
        "!pip install gradio transformers torch matplotlib\n",
        "import gradio as gr\n",
        "from transformers import DetrFeatureExtractor, DetrForObjectDetection\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import numpy as np\n",
        "\n",
        "# Load the model and feature extractor\n",
        "model_name = \"facebook/detr-resnet-50\"\n",
        "feature_extractor = DetrFeatureExtractor.from_pretrained(model_name)\n",
        "model = DetrForObjectDetection.from_pretrained(model_name)\n",
        "\n",
        "# Function to perform object detection and return an image with boxes\n",
        "def detect_objects(image):\n",
        "    # Prepare the image for the model\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "    # Perform inference\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Get the detected boxes and labels\n",
        "    target_sizes = torch.tensor([image.size[::-1]])  # Model expects (height, width)\n",
        "    results = feature_extractor.post_process(outputs, target_sizes=target_sizes)[0]\n",
        "\n",
        "    # Visualize the results\n",
        "    plt.imshow(image)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\n",
        "        if score > 0.9:  # Filter by confidence threshold\n",
        "            box = box.detach().numpy()\n",
        "            x, y, w, h = box\n",
        "            rect = Rectangle((x, y), w - x, h - y, linewidth=2, edgecolor='red', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            label_text = f\"{model.config.id2label[label.item()]}: {round(score.item(), 3)}\"\n",
        "            plt.text(x, y, label_text, color='white', fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n",
        "\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Save the image with bounding boxes\n",
        "    plt.savefig(\"detected_image.png\", bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return Image.open(\"detected_image.png\")\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=detect_objects,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=gr.Image(type=\"pil\"),\n",
        "    title=\"Object Detection\",\n",
        "    description=\"Upload an image and the model will detect objects with bounding boxes.\",\n",
        ")\n",
        "\n",
        "# Launch the app\n",
        "interface.launch()\n",
        "\n",
        "#Example 2.5\n",
        "# Generating the image of a Tourist Spot\n",
        "# Step 0\n",
        "! pip install gradio\n",
        "!pip install pydub\n",
        "\n",
        "# Step 1\n",
        "# Imports\n",
        "import gradio as gr\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import base64\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "import IPython.display as ipd\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('cm_muthu')\n",
        "openai = OpenAI(api_key = openai_api_key)\n",
        "\n",
        "# Step 2: Define a Function to generate an image of a Tourist Spot using DALL-E-3 Model\n",
        "def artist(city):\n",
        "   image_response = openai.images.generate(model = \"dall-e-3\",prompt = f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant style\",size = \"1024x1024\",n = 1,response_format = \"b64_json\", )\n",
        "   image_base64 = image_response.data[0].b64_json\n",
        "   image_data = base64.b64decode(image_base64)\n",
        "   return Image.open(BytesIO(image_data))\n",
        "\n",
        "# Step 3 : Generate an image of a Tourist Spot using DALL-E-3 Model\n",
        "image = artist(\"london\")\n",
        "display(image)\n",
        "\n",
        "\n",
        "#Example 2.6\n",
        "# Multimodal AI Assistant(Text, Image, and Audio are generated)\n",
        "# Lessons 62\n",
        "# Step 0\n",
        "! pip install gradio\n",
        "!pip install pydub\n",
        "\n",
        "# Step 1\n",
        "# Imports\n",
        "import gradio as gr\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import base64\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "import IPython.display as ipd\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('cm_muthu')\n",
        "openai = OpenAI(api_key = openai_api_key)\n",
        "\n",
        "# Step 1A: Define a function to get ticket price for destination city\n",
        "ticket_prices = {\"london\" : \"$799\", \"paris\" : \"$899\", \"tokyo\" : \"$1400\", \"berlin\" : \"$499\"}\n",
        "def get_ticket_price(destination_city):\n",
        "  # print(f\"Tool get_ticket_price called for {destination_city}\")\n",
        "  city = destination_city.lower()\n",
        "  return ticket_prices.get(city, \"Unknown\")\n",
        "\n",
        "# print(get_ticket_price(\"london\"))\n",
        "\n",
        "# Step 2: Define a function to generate an image\n",
        "def artist(city):\n",
        "   image_response = openai.images.generate(model = \"dall-e-3\",prompt = f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant style\",size = \"1024x1024\",n = 1,response_format = \"b64_json\", )\n",
        "   image_base64 = image_response.data[0].b64_json\n",
        "   image_data = base64.b64decode(image_base64)\n",
        "   return Image.open(BytesIO(image_data))\n",
        "# image = artist(\"london\")\n",
        "# display(image)\n",
        "\n",
        "# Step 3: Define a function to generate audio\n",
        "def talker (city , price):\n",
        "  message = f\"Ticket price for {city} is {price}\"\n",
        "  response = openai.audio.speech.create(model = \"tts-1\", voice = \"onyx\", input = message)\n",
        "  audio_stream = BytesIO(response.content)\n",
        "  audio = AudioSegment.from_file(audio_stream, format = \"mp3\")\n",
        "  audio.export(\"output.mp3\", format = \"mp3\")\n",
        "  play(audio)\n",
        "# talker(\"london\" , \"$799\")\n",
        "\n",
        "# Step 4: Define generate_output() function\n",
        "def generate_output(city_name):\n",
        "   ticket_price = get_ticket_price(city_name)\n",
        "   message = f\"Ticket price for {city_name} is {ticket_price}\"\n",
        "   image = artist(city_name)\n",
        "   talker(city_name , ticket_price)\n",
        "   return message , image\n",
        "# message, image = generate_output(\"london\")\n",
        "# display(image)\n",
        "\n",
        "# Step 5: Create Gradio Interface\n",
        "with gr.Blocks() as interface:\n",
        "  destination_city = gr.Textbox(label = \"Type Destination City name\", placeholder =\"e.g.London\")\n",
        "  ticket_price = gr.Textbox(label = \"Ticket Price\")\n",
        "  destination_city_image = gr.Image(label = \"Destination City Image\")\n",
        "  destination_city.submit(fn = generate_output, inputs = destination_city, outputs = [ticket_price, destination_city_image])\n",
        "\n",
        "interface.launch()'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hHutue1M48es"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}